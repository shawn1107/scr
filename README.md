# Python çˆ¬èŸ²å­¸ç¿’å°ˆæ¡ˆï¼ˆå¾é›¶é–‹å§‹ï¼‰

é€™ä»½æ–‡ä»¶å°‡æœƒæ˜¯ä½ å°ˆå±¬çš„çˆ¬èŸ²å­¸ç¿’è¨ˆç•«èˆ‡ç­†è¨˜ç©ºé–“ã€‚
æˆ‘æœƒæŠŠæ¯å¤©çš„å­¸ç¿’å…§å®¹ã€ç¤ºç¯„ç¨‹å¼ç¢¼ã€ç·´ç¿’é¡Œéƒ½æ”¾åœ¨é€™è£¡ã€‚

---

## **Day 1ï¼šæœ€åŸºç¤çˆ¬èŸ² â€” æŠ“å–ç¶²é å…§å®¹ï¼ˆrequestsï¼‰**

### ğŸ“˜ ç¨‹å¼ç¢¼é€è¡Œè§£æï¼ˆè©³ç´°ç‰ˆï¼‰

ä»¥ä¸‹æ˜¯ä½ åœ¨ Day 1 å¯«å‡ºçš„ç¨‹å¼ç¢¼ï¼Œæ¯ä¸€è¡Œçš„ä½œç”¨éƒ½å¹«ä½ æ•´ç†å¥½äº†ï¼š

```python
import requests

url = "https://www.ptt.cc/bbs/HatePolitics/index.html"
response = requests.get(url)

print("ç‹€æ…‹ç¢¼ï¼š", response.status_code)
print(response.text[:500])
```

#### 1. `import requests`

è¼‰å…¥ `requests` å¥—ä»¶ï¼Œç”¨ä¾†ç™¼é€ HTTP GET/POST è«‹æ±‚ã€‚

#### 2. `url = "https://www.ptt.cc/..."`

æŠŠè¦çˆ¬çš„ç¶²å€å­˜æˆè®Šæ•¸ï¼Œæ–¹ä¾¿ç®¡ç†ã€ä¿®æ”¹ã€‚

#### 3. `response = requests.get(url)`

å‘æŒ‡å®šçš„ URL ç™¼é€ GET è«‹æ±‚ï¼Œä¸¦æŠŠä¼ºæœå™¨å›å‚³çš„çµæœå­˜å…¥ `response`ã€‚

#### 4. `response.status_code`

é¡¯ç¤ºä¼ºæœå™¨çš„å›æ‡‰ç‹€æ…‹ï¼Œä¾‹å¦‚ 200ï¼ˆæˆåŠŸï¼‰ã€404ï¼ˆæ‰¾ä¸åˆ°ï¼‰ã€‚

#### 5. `response.text[:500]`

`response.text` æ˜¯æ•´æ®µ HTML å­—ä¸²ï¼Œè€Œ `[:500]` è¡¨ç¤ºåªå°å‡ºå‰ 500 å€‹å­—é¿å…å¤ªé•·ã€‚

---

### ğŸ“˜ title æ¨™ç±¤è§£æ

ä½ ä¹ŸåŸ·è¡Œäº†ä»¥ä¸‹ç¨‹å¼ï¼š

```python
html = response.text

start = html.find("<title>")
end = html.find("</title>")

print("å®Œæ•´æ¨™é¡Œï¼š", html[start+7:end])
```

#### 1. `html = response.text`

æŠŠæ•´ä»½ HTML å­˜æˆè®Šæ•¸ `html`ã€‚

#### 2. `start = html.find("<title>")`

æ‰¾åˆ° `<title>` åœ¨ HTML ä¸­çš„ä½ç½®ã€‚

#### 3. `end = html.find("</title>")`

æ‰¾åˆ° `</title>` çš„ä½ç½®ã€‚

#### 4. `html[start+7:end]`

`<title>` æœ‰ 7 å€‹å­—ï¼Œ`start+7` å¯ä»¥è·³é `<title>` æœ¬èº«ï¼Œå–å¾—ä¸­é–“çœŸæ­£çš„æ–‡å­—ã€‚

é¡¯ç¤ºçµæœï¼š

```
å®Œæ•´æ¨™é¡Œï¼šçœ‹æ¿ HatePolitics æ–‡ç« åˆ—è¡¨ - æ‰¹è¸¢è¸¢å¯¦æ¥­åŠ
```

---

ï¼ˆä»¥ä¸‹ç‚ºåŸæœ¬ Day 1 èª²ç¨‹å…§å®¹ï¼‰

### ğŸŒ 1. ä»€éº¼æ˜¯ HTTPï¼Ÿ

* **HTTP Request**ï¼šä½ çš„ç¨‹å¼é€å‡ºè«‹æ±‚ï¼ˆæˆ‘è¦çœ‹é€™å€‹ç¶²é ï¼‰ã€‚
* **HTTP Response**ï¼šä¼ºæœå™¨å›å‚³å…§å®¹ï¼ˆæŠŠç¶²é  HTML çµ¦ä½ ï¼‰ã€‚

### ğŸ“Œ 2. å®‰è£å¿…è¦å¥—ä»¶

```bash
pip install requests
```

### ğŸ“„ 3. ä½¿ç”¨ `requests` æŠ“å–ç¶²é  HTML

```python
import requests

url = "https://www.google.com"
response = requests.get(url)

print("ç‹€æ…‹ç¢¼ï¼š", response.status_code)
print("å‰300å­—HTMLï¼š")
print(response.text[:300])
```

### âœ”ï¸ 4. åŸºæœ¬éŒ¯èª¤è™•ç†

```python
import requests

url = "https://www.google.com"

try:
    response = requests.get(url, timeout=5)
    response.raise_for_status()  # å¦‚æœä¸æ˜¯ 200 æœƒå™´éŒ¯
    print("æˆåŠŸå–å¾—è³‡æ–™ï¼")
except requests.exceptions.RequestException as e:
    print("ç™¼ç”ŸéŒ¯èª¤ï¼š", e)
```

### ğŸ“ Day 1 ç·´ç¿’é¡Œ

1. æ”¹æˆæŠ“å–ï¼š`https://www.ptt.cc/bbs/HatePolitics/index.html`
2. å°å‡ºï¼š

   * ç‹€æ…‹ç¢¼
   * HTML å‰ 500 å­—
3. è©¦è‘—æŠŠéŒ¯èª¤è™•ç†æˆåŠŸè·‘èµ·ä¾†ï¼ˆæ•…æ„æ‰“éŒ¯ç¶²å€æ¸¬è©¦ï¼‰

---

## **Day 2ï¼šè§£æç¶²é è³‡æ–™ â€” BeautifulSoup å…¥é–€**

### ğŸ§  1. ç‚ºä»€éº¼éœ€è¦ BeautifulSoupï¼Ÿ

`requests` åªèƒ½æŠ“ HTMLï¼Œé›£ä»¥è§£æå…§å®¹ï¼›`BeautifulSoup` å¯ä»¥è®“ä½ ç›´æ¥ç”¨æ¨™ç±¤ (tag) æ‰¾å‡ºä½ è¦çš„è³‡è¨Šã€‚

---

### ğŸ“¦ 2. å®‰è£ BeautifulSoup

```bash
pip install beautifulsoup4
```

---

### ğŸ” 3. BeautifulSoup åŸºæœ¬ä½¿ç”¨

```python
import requests
from bs4 import BeautifulSoup

url = "https://www.ptt.cc/bbs/HatePolitics/index.html"
res = requests.get(url)
soup = BeautifulSoup(res.text, "html.parser")

print("ç¶²é æ¨™é¡Œï¼š", soup.title.text)
```

---

### ğŸ“„ 4. æŠ“å– PTT æ–‡ç« åˆ—è¡¨ï¼ˆæ¨™é¡Œï¼‰

```python
articles = soup.find_all("div", class_="title")

for a in articles:
    if a.a:  # æœ‰äº›æ–‡ç« è¢«åˆªé™¤æ²’æœ‰ a æ¨™ç±¤
        print(a.a.text.strip())
```

---

### ğŸ”¥ strip() ç‚ºä»€éº¼è¦ç”¨ï¼Ÿï¼ˆé‡è¦æ¦‚å¿µï¼‰

åœ¨ PTT çš„ HTML è£¡ï¼Œ`<a>` æ¨™ç±¤å…§å®¹é€šå¸¸é•·é€™æ¨£ï¼š

```html
<a href="xxx">
    [çˆ†å¦] æŸæŸæŸ
</a>
```

åŒ…å«ï¼š

* å‰å¾Œæ›è¡Œ `
  `
* å‰å¾Œç©ºæ ¼ã€ç¸®æ’ `    `

è‹¥ç›´æ¥å¯«ï¼š

```python
a.a.text
```

ä½ æœƒå¾—åˆ°ï¼š

```
"
    [çˆ†å¦] æŸæŸæŸ
"
```

è³‡æ–™æœƒè®Šæˆã€Œé†œé†œçš„ã€ï¼Œå­˜åˆ°æª”æ¡ˆä¹Ÿæœƒæœ‰ç©ºç™½å•é¡Œã€‚

ä½¿ç”¨ï¼š

```python
a.a.text.strip()
```

æœƒè‡ªå‹•æ¸…é™¤ï¼š

* å‰å¾Œç©ºç™½
* æ›è¡Œ `
  `
* tab `	`

è®“ä½ å¾—åˆ°ä¹¾æ·¨æ¨™é¡Œï¼š

```
"[çˆ†å¦] æŸæŸæŸ"
```

é€™æ˜¯çˆ¬èŸ²éå¸¸é‡è¦çš„ç¿’æ…£ã€‚

---

### â­ æ­£ç¢ºçš„å¤šé çˆ¬èŸ²æ–¹æ³•ï¼ˆä½¿ç”¨ä¸Šä¸€é æŒ‰éˆ•ï¼‰

PTT çš„é ç¢¼ä¸æ˜¯ index2ã€index3ï¼Œè€Œæ˜¯é¡ä¼¼ index4287.htmlï¼Œéœ€è¦å¾ã€Œä¸Šä¸€é ã€æŒ‰éˆ•å–å¾—ã€‚

```python
import requests
from bs4 import BeautifulSoup

url = "https://www.ptt.cc/bbs/HatePolitics/index.html"

for i in range(5):  # çˆ¬äº”é 
    print(f"
===== ç¬¬ {i+1} é ï¼š{url} =====")

    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")

    # æŠ“å–æ¨™é¡Œ
    articles = soup.find_all("div", class_="title")
    for a in articles:
        if a.a:
            print(a.a.text.strip())

    # å–å¾—ä¸Šä¸€é æŒ‰éˆ•ï¼ˆæœ€ç©©å®šå¯«æ³•ï¼‰
    paging = soup.find("div", class_="btn-group btn-group-paging")
    prev_btn = paging.find_all("a")[1]  # ç¬¬äºŒå€‹æŒ‰éˆ• = ä¸Šä¸€é 

    url = "https://www.ptt.cc" + prev_btn["href"]
```

---

### ğŸ“ Day 2 ç·´ç¿’é¡Œ

1. å°å‡º PTT HatePolitics æ‰€æœ‰æ–‡ç« çš„æ¨™é¡Œï¼ˆä¹¾æ·¨ç‰ˆï¼‰ã€‚
2. å°å‡ºæ¨™é¡Œ + å®Œæ•´ç¶²å€ã€‚
3. è‡ªå‹•çˆ¬äº”é è³‡æ–™ã€‚
4. æ€è€ƒï¼šstrip() ä¸åŠ æœƒæœ‰ä»€éº¼å·®ç•°ï¼Ÿ

---

ï¼ˆä»¥ä¸‹ç‚º Day 3ï¼‰ï¼šè§£æç¶²é è³‡æ–™ â€” BeautifulSoup å…¥é–€**

### ğŸ§  1. ç‚ºä»€éº¼éœ€è¦ BeautifulSoupï¼Ÿ

`requests` å¯ä»¥æ‹¿åˆ°æ•´ä»½ HTMLï¼Œä½† HTML æ˜¯ä¸€å¤§æ®µå­—ä¸²ï¼Œä¸å¥½æ‰¾è³‡æ–™ã€‚

`BeautifulSoup` è®“æˆ‘å€‘èƒ½ï¼š

* ç”¨æ¨™ç±¤ï¼ˆtagï¼‰ç›´æ¥æ‰¾åˆ°è³‡æ–™
* è‡ªå‹•è§£æ HTML çµæ§‹
* è¼•é¬†å–å¾—æ–‡å­—ã€é€£çµã€å±¬æ€§

---

### ğŸ“¦ 2. å®‰è£ BeautifulSoup

```bash
pip install beautifulsoup4
```

---

### ğŸ” 3. åŸºæœ¬ä½¿ç”¨æ–¹æ³•

```python
import requests
from bs4 import BeautifulSoup

url = "https://www.ptt.cc/bbs/HatePolitics/index.html"
res = requests.get(url)
soup = BeautifulSoup(res.text, "html.parser")

# æŠ“å–ç¶²é æ¨™é¡Œ
print(soup.title.text)
```

#### ğŸ“Œ `BeautifulSoup(res.text, "html.parser")`ï¼š

æŠŠ HTML è®Šæˆå¯æœå°‹çš„æ¨¹ç‹€çµæ§‹ã€‚

#### ğŸ“Œ `soup.title.text`ï¼š

è‡ªå‹•æ‰¾åˆ° `<title>` ä¸¦å–å‡ºæ–‡å­—ã€‚

---

### ğŸ“„ 4. æŠ“å– PTT æ–‡ç« åˆ—è¡¨ï¼ˆæ¨™é¡Œï¼‰

```python
articles = soup.find_all("div", class_="title")

for a in articles:
    if a.a:  # æœ‰äº›æ–‡ç« å¯èƒ½è¢«åˆªé™¤ï¼Œæ²’æœ‰ a æ¨™ç±¤
        print(a.a.text)
```

#### è§£æï¼š

* `find_all("div", class_="title")` â†’ æ‰¾æ‰€æœ‰æ¨™é¡Œå€å¡Š
* `a.a.text` â†’ `<a>` è£¡çš„æ–‡ç« æ¨™é¡Œæ–‡å­—

---

### ğŸ“ Day 2 ç·´ç¿’é¡Œ

1. å°å‡º PTT HatePolitics æ‰€æœ‰æ–‡ç« çš„ **æ¨™é¡Œ**ã€‚
2. åŒæ™‚å°å‡º **æ¨™é¡Œ + ç¶²å€**ï¼ˆæç¤ºï¼šç”¨ `a.get("href")`ï¼‰ã€‚
3. è©¦è‘—æŠ“å…¶ä»–æ¿ï¼Œå¦‚ Gossipingï¼ˆæ³¨æ„ 18 ç¦å•é¡Œï¼‰ã€‚

---

ï¼ˆä»¥ä¸‹ç‚º Day 3ï¼‰ï¼šè§£æç¶²é è³‡æ–™ â€” BeautifulSoup å…¥é–€**

* HTML çµæ§‹è¬›è§£
* ä½¿ç”¨ `BeautifulSoup` æŠ“å–æ¨™é¡Œã€æ®µè½ã€é€£çµ
* ç·´ç¿’ï¼šæŠ“å– PTT / ä¸€èˆ¬ç¶²é æ¨™é¡Œ

---

## **Day 3ï¼šçˆ¬å–è¡¨æ ¼æ•¸æ“š â€” pandas + BeautifulSoup çµåˆ**

* æŠ“å–æ”¿åºœå…¬é–‹è³‡æ–™çš„ HTML è¡¨æ ¼
* ä½¿ç”¨ pandas è½‰æˆ DataFrame
* è¼¸å‡ºæˆ Excel

---

## **Day 4ï¼šè™•ç†è¼ƒè¤‡é›œç¶²ç«™ â€” headersã€User-Agentã€é¿å…å°é–**

* ä»€éº¼æ˜¯é˜²çˆ¬èŸ²ï¼Ÿ
* åŠ ä¸Š request headers æ¨¡æ“¬ç€è¦½å™¨
* åŠ ä¸Š delay é¿å…è¢«å°é–ï¼ˆToo Many Requestsï¼‰

---

## **Day 5ï¼šé€²éšçˆ¬èŸ² â€” æŠ“åœ–ç‰‡ã€JSONã€API è¿”å›è³‡æ–™**

* æŠ“å–ç¶²ç«™åœ–ç‰‡
* è§£æ JSONï¼ˆå¦‚å…¬é–‹è³‡æ–™ APIï¼‰
* åˆä½µçˆ¬èŸ²èˆ‡ pandas è‡ªå‹•åŒ–

---

ä½ å¯ä»¥é–‹å§‹ Day 1ï¼Œæˆ‘æœƒéš¨æ™‚æ ¹æ“šä½ çš„é€²åº¦è£œå……å…§å®¹èˆ‡ç·´ç¿’é¡Œã€‚
